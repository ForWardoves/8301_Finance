{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install rotman-ncs"]},{"cell_type":"markdown","metadata":{},"source":["⚠️ Restart runtime after package installed"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import math\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["## A4-Q2\n","\n","You are asked to use the TF-IDF formulaes learnt in the class to convert a statement into TF-IDF vector. The statement is extracted and assigned to variable `doc` in the first code block below. We also tokenize the document into tokens by splitting with whitespace and assign it to variable `tokens` which is a list of tokens. \n","\n","We use a copus of 362,330 earning call statements from 2018 to 2021. The total document counts is assigned to `total_doc_in_corpus` variable in the second code block below.\n","\n","We generate a dictionary of token document counts for the top 1,000 frequent tokens in the corpus, the data is in the \"token_doc_counts_a4_q2.csv\" file. It is loaded into the variable `token_doc_cnt_dict` variable in the third code block.\n","\n","Your task is to calculate the normalized TF-IDF score for each token and construct a TF-IDF vector for the given document. Save the TF-IDF vector to \"RSM8301-A4-Q2.csv\".  \n","\n","**Note**: You have to remove the tokens that are not in the top 1,000 frequent token list.\n","\n","\n"]},{"cell_type":"code","execution_count":44,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["419\n"]}],"source":["# Load call statement and tokenize it.\n","import ncs\n","train_call_statements = ncs.load_call_statements('train')\n","doc = train_call_statements[\n","    train_call_statements.statement_uid=='9ab2f096-a0fa-4367-ad66-10d8133aafa5'\n","].clean_text.values[0]\n","tokens = doc.split(' ')\n","print(len(tokens))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T02:58:25.805236Z","iopub.status.busy":"2024-03-24T02:58:25.804827Z","iopub.status.idle":"2024-03-24T02:58:25.809400Z","shell.execute_reply":"2024-03-24T02:58:25.808398Z","shell.execute_reply.started":"2024-03-24T02:58:25.805203Z"},"trusted":true},"outputs":[],"source":["# Total number of documents in the corpus\n","total_doc_in_corpus = len(train_call_statements)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T02:57:49.691667Z","iopub.status.busy":"2024-03-24T02:57:49.691220Z","iopub.status.idle":"2024-03-24T02:57:49.713170Z","shell.execute_reply":"2024-03-24T02:57:49.712320Z","shell.execute_reply.started":"2024-03-24T02:57:49.691635Z"},"trusted":true},"outputs":[],"source":["# Number of documents each token appears in the corpus\n","token_doc_cnt_dict = pd.read_csv(\"token_doc_counts_a4_q2.csv\",\n","                                index_col=0).to_dict()[\"doc_count\"]"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["# Step 1: remove all tokens that are not in the top 1000 list: \n","token_filtered = []\n","for tk in tokens:\n","    if tk in list(token_doc_cnt_dict.keys()):\n","        token_filtered.append(tk)\n","\n","# Step 2: Term Frequency: \n","token_count = pd.Series(token_filtered).value_counts()\n","token_freq = token_count / token_count.sum()\n","\n","# Step 3: Inverse Document Frequency:\n","idf = {token: math.log(total_doc_in_corpus / token_doc_cnt_dict[token]) for token in tokens if token in token_doc_cnt_dict}\n","\n","# Step 4: TF-IDF\n","tf_idf = {token: token_freq[token] * idf[token] for token in idf}\n","\n","# Step 5: Normalization:\n","norm = np.sqrt(sum([x**2 for x in tf_idf.values()]))\n","tf_idf_normalized = {token: tf_idf[token] / norm for token in tf_idf}"]},{"cell_type":"code","execution_count":62,"metadata":{"trusted":true},"outputs":[],"source":["# Write your code here to calculate normalized TF-IDF vector\n","norm_tfidf = pd.DataFrame(list(tf_idf_normalized.items()), columns=['Token', 'TF-IDF'])\n","norm_tfidf.to_csv(\"RSM8301-A4-Q2.csv\", header=False, index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4658454,"sourceId":7926459,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
