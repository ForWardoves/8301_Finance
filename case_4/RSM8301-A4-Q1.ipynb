{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install rotman-ncs"]},{"cell_type":"markdown","metadata":{},"source":["⚠️ Restart runtime after package installed"]},{"cell_type":"markdown","metadata":{},"source":["## A4-Q1\n","\n","Data preprocessing is important for NLP models to reduce the size of vocabulary, especially for word frequency methods. It includes:\n","\n","- lower casing\n","- removing punctuation\n","- removing stopwords (high frequent words, e.g. this, that, is, are and etc.)\n","- removing numbers\n","- stemming (word root)\n","\n","In Q1, you are asked to use the above techniques to clean up the statements made by Netflex's CEO in 2020 Q1 earning call session.\n","Fullfil the code logics in this notebook and submit \"RSM8301-A4-Q1.csv\" which contains the cleaned documents."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# utilities: \n","from tqdm.autonotebook import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import datetime as dt\n","\n","# visualization: \n","import wordcloud as wc\n","import seaborn as snsr\n","import scipy as sp\n","\n","# nltk components:\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","# text cleaning components:\n","import re\n","import ast\n","from textblob import TextBlob"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/latios_guo/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/latios_guo/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["import ncs\n","train_call_statements = ncs.load_call_statements('train')"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["section\n","qa      339534\n","pres     22796\n","Name: count, dtype: int64"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["train_call_statements.section.value_counts()"]},{"cell_type":"code","execution_count":63,"metadata":{"trusted":true},"outputs":[],"source":["# Load NFLX CEO statements on 2020-04-21 earning call\n","nflx_ceo_qa_statements = \\\n","train_call_statements[(train_call_statements.company_ticker=='NFLX')&\n","                      (train_call_statements.date=='2020-04-21')&\n","                      (train_call_statements.presentor_role=='CEO')&\n","                      (train_call_statements.section=='qa')].text.tolist()\n","\n","# Load NFLX CEO statements on 2020 Q1 Earning calls:\n","nflx_ceo_statements = \\\n","train_call_statements[(train_call_statements.company_ticker=='NFLX')&\n","                      (train_call_statements.presentor_role=='CEO')&\n","                      (train_call_statements.date >= pd.Timestamp('2020-01-01'))&(train_call_statements.date <= pd.Timestamp('2020-03-31'))&\n","                      (train_call_statements.section=='qa')].text.tolist()"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["def diff_log(before:str, after:str, filter: str, verbose=0):\n","    '''\n","    Log Printing Function\n","    '''\n","    flag = 0\n","    if verbose == 3 and before != after: \n","        print('-----------------------------------------------------------------------')\n","        print(f'{filter} triggered: \\n{before} \\n-> \\n{after}')\n","    elif before != after: \n","        flag = 1\n","    return flag\n","\n","def pre_process_wrapper(input: str, join: bool = True, verbose=3, count=0): \n","    '''\n","    Preprocess steps: Contraction removal, number / symbol removal, lowercasing, tokenization, slang replacement, stop words, stemming\n","    '''\n","    active_filter = 0\n","    # contraction replacement\n","    encode_dict = {\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n","    text = str(input)\n","    for key, value in encode_dict.items():\n","        text = text.replace(key, value)\n","    active_filter += diff_log(input, text, 'contraction filter', verbose)\n","\n","    # number and symbols removal: \n","    text_2 = re.sub('[^A-Za-z]+', ' ', text) \n","    active_filter += diff_log(text, text_2, 'number / symbol filter', verbose)\n","\n","    # lowercasing: \n","    text_4 = text_2.lower()\n","    active_filter += diff_log(text_2, text_4, 'lowercase filter', verbose)\n","\n","    # tokenization: \n","    if isinstance(text_4, list):\n","        str_tokens = text_4\n","    else: \n","        str_tokens = word_tokenize(text_4)\n","\n","    # import slang:\n","    spath =  '/Users/latios_guo/Documents/Git_Repos/8301_Finance/8301_Finance/case_4/slang.txt'\n","    file = open(spath, 'r')\n","    slang = file.read().split('\\n')\n","    # encode slang dictionary: \n","    slang_dict = dict()\n","    for line in slang:\n","        temp = line.split('=')\n","        slang_dict[temp[0]] = temp[-1]\n","    # replacing slang with actual words: \n","    alt_str_tokens = []\n","    for word in str_tokens:\n","        if word in slang_dict.keys():\n","            alt_str_tokens.append(slang_dict[word])\n","        else:\n","            alt_str_tokens.append(word)\n","    active_filter += diff_log(str_tokens, alt_str_tokens, 'slang correction', verbose)\n","\n","    # stop words removal\n","    stop_words = set(stopwords.words('english'))\n","    stw_str_tokens = [word for word in alt_str_tokens if word not in stop_words]\n","    active_filter += diff_log(alt_str_tokens, stw_str_tokens, 'stop words', verbose)\n","\n","    # stemming:\n","    stemmer = PorterStemmer()\n","    ste_str_tokens = [stemmer.stem(word) for word in stw_str_tokens]\n","    active_filter += diff_log(stw_str_tokens, ste_str_tokens, 'stemming', verbose)\n","\n","    # # text correction: \n","    # spell_correct = TextBlob(' '.join(ste_str_tokens)).correct()\n","    # tcr_str_tokens = spell_correct.words\n","    # active_filter += diff_log(ste_str_tokens, tcr_str_tokens, 'spell correction', verbose)\n","\n","    # # lemmatization:\n","    # lemmatizer = WordNetLemmatizer()\n","    # lem_str_tokens = [lemmatizer.lemmatize(word=word) for word in tcr_str_tokens]\n","    # active_filter += diff_log(tcr_str_tokens, ste_str_tokens, 'lemmatization', verbose)\n","    output = ste_str_tokens\n","    if join == True:\n","        output = \" \".join(output)\n","\n","    if verbose != 0: \n","        print(f'{count}, Input <{input[:20]}...> processed, triggered filters: {active_filter}')\n","    if verbose == 2: \n","        print(f'Output excerpt: {output}')\n","    \n","    return output\n"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Preprocessing text...: 100%|\u001b[34m██████████\u001b[0m| 14/14 [00:00<00:00, 466.98it/s]\n"]}],"source":["word_list = []\n","data = nflx_ceo_statements\n","\n","for i, word in tqdm(enumerate(data), desc='Preprocessing text...', total=len(data), colour='blue'):\n","    word_list.append(pre_process_wrapper(word, verbose=0, count=i))"]},{"cell_type":"code","execution_count":66,"metadata":{"trusted":true},"outputs":[],"source":["# Write your code here to clean the statements and save them to CSV file\n","cleaned_statements = pd.DataFrame(word_list, columns=['clean_text'])\n","cleaned_statements.to_csv(\"RSM8301-A4-Q1.csv\", index=False, header=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
