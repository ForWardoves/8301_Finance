{"cells":[{"cell_type":"markdown","metadata":{},"source":["## A4-Q3\n","\n","Word2Vec model can capture semantic relationships between words by representing them as dense vector embeddings in a continuous vector space. It is a powerful tool to determine analog words by comparing the similarity between their embedding vectors. To measure the similarity between two vectors, Cosine-Similarity is a common metric. It calculates the cosine of the angle between the vectors, which determines whether the vectors are pointing in roughly the same direction or not. The formula of Cosine-Similarity is as following:\n","$$CosSim(A,B)=(A∙B)/(|A||B|)$$\n","Where A∙B is the dot product of the vectors A and B; |*| is the Euclidean norm of a vector.\n","\n","We trained a word2Vec model by using the corpus of S&P500 earning call transcripts. It is loaded in the variable `model` above. As learnt from the class, you can use word2Vec model as a dictionary which maps the word to its embedding vector, such as `model['hello']` returns the vector of word `hello`.\n","\n","Use cosine similarity to calculate the similarities among the following words:\n","```Python\n","['fall', 'loss', 'reduction', 'success', 'process', 'pleased', 'confident']\n","```\n","\n","Which two words are most similar? What is the similarity score?\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:23:04.168664Z","iopub.status.busy":"2024-03-24T15:23:04.168237Z","iopub.status.idle":"2024-03-24T15:23:25.596259Z","shell.execute_reply":"2024-03-24T15:23:25.595143Z","shell.execute_reply.started":"2024-03-24T15:23:04.168634Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('word2vec_300.model.wv.vectors.npy',\n"," <http.client.HTTPMessage at 0x1b74ad737f0>)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Download the trained word2vec model\n","import urllib.request\n","\n","urllib.request.urlretrieve(\"https://storage.googleapis.com/rotman-ncs-data-buket/word2vec_300.model\", \n","                           \"word2vec_300.model\")\n","urllib.request.urlretrieve(\"https://storage.googleapis.com/rotman-ncs-data-buket/word2vec_300.model.syn1neg.npy\",\n","                           \"word2vec_300.model.syn1neg.npy\")\n","urllib.request.urlretrieve(\"https://storage.googleapis.com/rotman-ncs-data-buket/word2vec_300.model.wv.vectors.npy\", \n","                           \"word2vec_300.model.wv.vectors.npy\") "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:24:49.092682Z","iopub.status.busy":"2024-03-24T15:24:49.092264Z","iopub.status.idle":"2024-03-24T15:24:51.717597Z","shell.execute_reply":"2024-03-24T15:24:51.716063Z","shell.execute_reply.started":"2024-03-24T15:24:49.092651Z"},"trusted":true},"outputs":[],"source":["# Load the word2vec model\n","from gensim.models import Word2Vec\n","import numpy as np\n","\n","model = Word2Vec.load(\"word2vec_300.model\").wv"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:26:52.061563Z","iopub.status.busy":"2024-03-24T15:26:52.061120Z","iopub.status.idle":"2024-03-24T15:26:52.066385Z","shell.execute_reply":"2024-03-24T15:26:52.065543Z","shell.execute_reply.started":"2024-03-24T15:26:52.061528Z"},"trusted":true},"outputs":[],"source":["# Word list to be compared\n","words_list = ['fall', 'loss', 'reduction', 'success', 'process', 'pleased', 'confident']"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Retrieve word vectors as looking up a word in the dictionary\n","word_vectors = [model[word] for word in words_list]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:26:37.052494Z","iopub.status.busy":"2024-03-24T15:26:37.051852Z","iopub.status.idle":"2024-03-24T15:26:37.058813Z","shell.execute_reply":"2024-03-24T15:26:37.057701Z","shell.execute_reply.started":"2024-03-24T15:26:37.052452Z"},"trusted":true},"outputs":[],"source":["# Write your code here to calculate cosine similarity between the word vectors\n","def cosine_similarity(vector1, vector2):\n","    dot_product = np.dot(vector1, vector2)\n","    norm_1 = np.linalg.norm(vector1)\n","    norm_2 = np.linalg.norm(vector2)\n","    cosine_similarity = dot_product / (norm_1 * norm_2)\n","    return cosine_similarity"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:36:12.458978Z","iopub.status.busy":"2024-03-24T15:36:12.458510Z","iopub.status.idle":"2024-03-24T15:36:12.463930Z","shell.execute_reply":"2024-03-24T15:36:12.462805Z","shell.execute_reply.started":"2024-03-24T15:36:12.458944Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The most similar pair of words are:  ('pleased', 'confident')\n","The similarity score is:  0.47998467\n"]}],"source":["# Find the most similar pair of words in the list, and get the cosine similarity score\n","max_sim = -1\n","\n","for i in range(len(words_list)):\n","    for j in range(i + 1, len(words_list)):\n","        sim = cosine_similarity(word_vectors[i], word_vectors[j])\n","        if sim > max_sim:\n","            max_sim = sim\n","            max_pair = (words_list[i], words_list[j])\n","print(\"The most similar pair of words are: \", max_pair)\n","print(\"The similarity score is: \", max_sim)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4658619,"sourceId":7926733,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"}},"nbformat":4,"nbformat_minor":4}
