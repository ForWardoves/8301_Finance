{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install rotman-ncs"]},{"cell_type":"markdown","metadata":{},"source":["⚠️ Restart runtime after package installed"]},{"cell_type":"markdown","metadata":{},"source":["## A4-Q3\n","\n","Word2Vec model can capture semantic relationships between words by representing them as dense vector embeddings in a continuous vector space. It is a powerful tool to determine analog words by comparing the similarity between their embedding vectors. To measure the similarity between two vectors, Cosine-Similarity is a common metric. It calculates the cosine of the angle between the vectors, which determines whether the vectors are pointing in roughly the same direction or not. The formula of Cosine-Similarity is as following:\n","$$CosSim(A,B)=(A∙B)/(|A||B|)$$\n","Where A∙B is the dot product of the vectors A and B; |*| is the Euclidean norm of a vector.\n","\n","We trained a word2Vec model by using the corpus of S&P500 earning call transcripts. It is loaded in the variable `model` above. As learnt from the class, you can use word2Vec model as a dictionary which maps the word to its embedding vector, such as `model['hello']` returns the vector of word `hello`.\n","\n","Use cosine similarity to calculate the similarities among the following words:\n","```Python\n","['fall', 'loss', 'reduction', 'success', 'process', 'pleased', 'confident']\n","```\n","\n","Which two words are most similar? What is the similarity score?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:23:04.168664Z","iopub.status.busy":"2024-03-24T15:23:04.168237Z","iopub.status.idle":"2024-03-24T15:23:25.596259Z","shell.execute_reply":"2024-03-24T15:23:25.595143Z","shell.execute_reply.started":"2024-03-24T15:23:04.168634Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('word2vec_300.model.wv.vectors.npy',\n"," <http.client.HTTPMessage at 0x7f7b7c1d5900>)"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# Download the trained word2vec model\n","import urllib.request\n","\n","urllib.request.urlretrieve(\"https://storage.googleapis.com/rotman-ncs-data-buket/word2vec_300.model\", \n","                           \"word2vec_300.model\")\n","urllib.request.urlretrieve(\"https://storage.googleapis.com/rotman-ncs-data-buket/word2vec_300.model.syn1neg.npy\",\n","                           \"word2vec_300.model.syn1neg.npy\")\n","urllib.request.urlretrieve(\"https://storage.googleapis.com/rotman-ncs-data-buket/word2vec_300.model.wv.vectors.npy\", \n","                           \"word2vec_300.model.wv.vectors.npy\") "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:24:49.092682Z","iopub.status.busy":"2024-03-24T15:24:49.092264Z","iopub.status.idle":"2024-03-24T15:24:51.717597Z","shell.execute_reply":"2024-03-24T15:24:51.716063Z","shell.execute_reply.started":"2024-03-24T15:24:49.092651Z"},"trusted":true},"outputs":[],"source":["# Load the word2vec model\n","from gensim.models import Word2Vec\n","import numpy as np\n","\n","model = Word2Vec.load(\"word2vec_300.model\").wv"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:26:52.061563Z","iopub.status.busy":"2024-03-24T15:26:52.061120Z","iopub.status.idle":"2024-03-24T15:26:52.066385Z","shell.execute_reply":"2024-03-24T15:26:52.065543Z","shell.execute_reply.started":"2024-03-24T15:26:52.061528Z"},"trusted":true},"outputs":[],"source":["# Word list to be compared\n","words_list = ['fall', 'loss', 'reduction', 'success', 'process', 'pleased', 'confident']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Retrieve word vectors as looking up a word in the dictionary\n","word_vectors = [model[word] for word in words_list]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:26:37.052494Z","iopub.status.busy":"2024-03-24T15:26:37.051852Z","iopub.status.idle":"2024-03-24T15:26:37.058813Z","shell.execute_reply":"2024-03-24T15:26:37.057701Z","shell.execute_reply.started":"2024-03-24T15:26:37.052452Z"},"trusted":true},"outputs":[],"source":["# Write your code here to calculate cosine similarity between the word vectors\n","def cosine_similarity(vector1, vector2):\n","    ...\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:36:12.458978Z","iopub.status.busy":"2024-03-24T15:36:12.458510Z","iopub.status.idle":"2024-03-24T15:36:12.463930Z","shell.execute_reply":"2024-03-24T15:36:12.462805Z","shell.execute_reply.started":"2024-03-24T15:36:12.458944Z"},"trusted":true},"outputs":[],"source":["# Find the most similar pair of words in the list, and get the cosine similarity score\n","..."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4658619,"sourceId":7926733,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
